{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJrXJxJolHK+74qAt2nHWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag314/Practice/blob/main/Nested_CV_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9YYz2pjwm97g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "X,y=load_iris(return_X_y=True)"
      ],
      "metadata": {
        "id": "HQBrgO_nnOYC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outer loop splits the data into K folds, typically using k-fold cross-validation. Each fold serves as a hold-out validation set while the rest of the data forms the training set. The model is evaluated K times, each time using a different fold as the validation set. The final performance score is the average of the scores obtained from all the iterations."
      ],
      "metadata": {
        "id": "MqUVj-nXnYXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "outer_folds=5\n",
        "\n",
        "outer_cv=KFold(n_splits=outer_folds,shuffle=True,random_state=42)\n",
        "\n",
        "outer_scores=[]"
      ],
      "metadata": {
        "id": "Axf0H82SnXyD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inner Cross-Validation Loop (Hyperparameter Tuning)\n",
        "The inner loop is used for hyperparameter tuning. It also uses k-fold cross-validation to select the best hyperparameters for the model. For each set of hyperparameters, the model is trained on the training set (from the outer loop) and evaluated on the validation set (from the outer loop). The best hyperparameters are determined based on the average performance across all the inner fold iterations."
      ],
      "metadata": {
        "id": "8dEBk1uBnuL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid={'C':[0.1,1,10],\n",
        "            'kernel':['linear','rbf']}\n",
        "#initialize inner cv grid search\n",
        "inner_cv=KFold(n_splits=3,shuffle=True,random_state=42)\n",
        "\n",
        "svm_clf=SVC()\n",
        "\n",
        "grid_search=GridSearchCV(estimator=svm_clf,param_grid=param_grid,cv=inner_cv)"
      ],
      "metadata": {
        "id": "JYnn3Nnqnv4V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform nested CV: we loop through the outer folds and for each fold, we perform hyperparameter tuing using the inner CV. We then train the model on the training set(excluding the validation fold) and evaluate its performance on the validation set."
      ],
      "metadata": {
        "id": "-FRhlRYFoPtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nested CV\n",
        "for train_index,test_index in outer_cv.split(X):\n",
        "  X_train,X_test=X[train_index],X[test_index]\n",
        "  y_train,y_test=y[train_index],y[test_index]\n",
        "\n",
        "  #hyperparameter tuning using inner CV\n",
        "  grid_search.fit(X_train,y_train)\n",
        "\n",
        "  # Get the best estimator with tuned hyperparamets\n",
        "  best_svm_clf=grid_search.best_estimator_\n",
        "\n",
        "  # valuate on validation set\n",
        "  score=best_svm_clf.score(X_test,y_test)\n",
        "  outer_scores.append(score)\n",
        "\n",
        "# Final score\n",
        "mean_score=np.mean(outer_scores)\n",
        "print(\"NEsted CV mean accuracy:\", mean_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK1mNyB0nTli",
        "outputId": "6a28b652-fac9-44be-886c-a43cd36c50c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEsted CV mean accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vMorF9YMvWn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RbnJc2D4vWka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine nested CV with different seeds with same base learner"
      ],
      "metadata": {
        "id": "OSnRQrVgvWaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X,y=load_iris(return_X_y=True)\n",
        "\n",
        "outer_folds=5\n",
        "outer_cv=KFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
        "outer_socres=[]\n",
        "\n",
        "seed_values=[123,234,345]\n",
        "\n",
        "for trian_index,test_index in outer_cv.split(X):\n",
        "  X_train,X_test=X[train_index],X[test_index]\n",
        "  y_train,y_test=y[train_index],y[test_index]\n",
        "\n",
        "  prediction=[]\n",
        "\n",
        "  for seed in seed_values:\n",
        "    base_learner=RandomForestClassifier(random_state=seed)\n",
        "\n",
        "    base_learner.fit(X_train,y_train)\n",
        "\n",
        "    y_pred=base_learner.predict(X_test)\n",
        "\n",
        "    prediction.append(y_pred)\n",
        "\n",
        "  ensemble_predictins=sum(prediction)/len(prediction)\n",
        "\n",
        "  accuracy=np.mean(ensemble_predictins==y_test)\n",
        "\n",
        "  outer_scores.append(accuracy)\n",
        "\n",
        "mean_accuracy=np.mean(outer_scores)\n",
        "\n",
        "print(\"Nested CV mean accuracy:\",mean_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBfUdNBGvhIJ",
        "outputId": "0fc4cd2c-657d-4fcb-8fab-3f71cead0741"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested CV mean accuracy: 0.9833333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the nested cross-validation technique for model selection and hyperparameter tuning, with the ensemble of a single base learner using different seeds."
      ],
      "metadata": {
        "id": "LGXQP1NXx32M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X,y=load_iris(return_X_y=True)\n",
        "\n",
        "outer_folds=5\n",
        "outer_cv=KFold(n_splits=outer_folds,shuffle=True, random_state=42)\n",
        "outer_scores=[]\n",
        "\n",
        "#Define different models\n",
        "\n",
        "models={\n",
        "    'RandomForest':(RandomForestClassifier(),{\n",
        "        'n_estimators':[50,100,150],\n",
        "        'max_depth':[None,5,10],\n",
        "        'min_samples_split':[2,5]\n",
        "    } ),\n",
        "    'SVM':(SVC(),{\n",
        "        'C':[1,10,100],\n",
        "        'kernel':['linear','rbf']\n",
        "    }),\n",
        "    'GradientBoosting':(GradientBoostingClassifier(),{\n",
        "        'n_estimators':[50,100,150],\n",
        "        'learning_rate':[0.1,0.01],\n",
        "        'max_depth':[3,5]\n",
        "\n",
        "    })\n",
        "}\n",
        "\n",
        "seed_values=[324,562,987]\n",
        "\n",
        "for train_index,test_index in outer_cv.split(X):\n",
        "  X_train,X_test=X[train_index],X[test_index]\n",
        "  y_train,y_test=y[train_index],y[test_index]\n",
        "\n",
        "  predictions=[]\n",
        "\n",
        "  for model_name,(model, param_grid) in models.items():\n",
        "    inner_cv=KFold(n_splits=3,shuffle=True, random_state=42)\n",
        "    grid_search=GridSearchCV(estimator=model, param_grid=param_grid,cv=inner_cv)\n",
        "\n",
        "    grid_search.fit(X_train,y_train)\n",
        "    best_model=grid_search.best_estimator_\n",
        "\n",
        "    model_predictions=[]\n",
        "\n",
        "    for seed in seed_values:\n",
        "      base_learner=best_model.__class__(**best_model.get_params())\n",
        "      base_learner.set_params(random_state=seed)\n",
        "      base_learner.fit(X_train,y_train)\n",
        "      y_pred=base_learner.predict(X_test)\n",
        "      model_predictions.append(y_pred)\n",
        "\n",
        "    ensemble_predictions=sum(model_predictions)/len(model_predictions)\n",
        "    predictions.append(ensemble_predictions)\n",
        "  accuracies=[np.mean(pred==y_test) for pred in predictions]\n",
        "  outer_scores.append(accuracies)\n",
        "\n",
        "mean_scores=np.mean(outer_scores,axis=0)\n",
        "model_names=list(models.keys())\n",
        "\n",
        "for name,score in zip(model_names,mean_scores):\n",
        "  print(f\"{name} - Mean Accuracy : {score: .4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR8CI8M5x5qH",
        "outputId": "a9095c9a-b176-4095-daf6-d6933da8a0e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest - Mean Accuracy :  0.9533\n",
            "SVM - Mean Accuracy :  0.9667\n",
            "GradientBoosting - Mean Accuracy :  0.9533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search exhaustively explores the hyperparameter space, it can be computationally expensive. Random Search is less intensive but may not find the optimal combination. Bayesian Optimization strikes a balance between the two by using probabilistic models to guide the search intelligently."
      ],
      "metadata": {
        "id": "QXVJldXqlBot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83mK_sjlp2R",
        "outputId": "1bfb8b9f-e7dc-47f8-ae1c-05c822838f97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/100.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-23.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.2.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.7.0 scikit-optimize-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bayesian search optimization\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "#load the data\n",
        "iris=load_iris()\n",
        "X,y=iris.data, iris.target\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "\n",
        "param_space={\n",
        "    'C':(0.01,10.0,'log-uniform'),\n",
        "    'gamma':(0.001,1.0,'log-uniform'),\n",
        "    'kernel':['linear','poly','rbf']\n",
        "}\n",
        "\n",
        "#initilize bayesian object\n",
        "optimal_params=BayesSearchCV(\n",
        "    SVC(),\n",
        "    param_space,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimal_params.fit(X_train,y_train)\n",
        "#Perform optimization to find best params\n",
        "print(\"Best hyperparameters: \",optimal_params.best_params_)\n",
        "print(\"Best score :\", optimal_params.best_score_)\n",
        "\n",
        "#Evaluate optimized model on test set\n",
        "best_svm=optimal_params.best_estimator_\n",
        "test_score=best_svm.score(X_test,y_test)\n",
        "\n",
        "print(\"Test set accuracy :\", test_score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrCjG8KwlIUB",
        "outputId": "208c4b22-3b75-489d-914b-95c4fc8a0487"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  OrderedDict([('C', 1.5925169151603527), ('gamma', 0.6578232405415442), ('kernel', 'linear')])\n",
            "Best score : 0.9583333333333334\n",
            "Test set accuracy : 1.0\n"
          ]
        }
      ]
    }
  ]
}